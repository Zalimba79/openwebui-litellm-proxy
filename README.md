ğŸ‡©ğŸ‡ª Deutsch:
openwebui-litellm-proxy ist ein Setup-Projekt zur Integration von LiteLLM als universelle API-Schnittstelle fÃ¼r verschiedene LLM-Anbieter in OpenWebUI.
Es ermÃ¶glicht die Anbindung von Modellen wie OpenAI, Mistral, Ollama, Anthropic u.â€¯v.â€¯m. Ã¼ber eine zentrale Proxy-Konfiguration.

ğŸ‡¬ğŸ‡§ English:
openwebui-litellm-proxy is a setup project to integrate LiteLLM as a universal API gateway for multiple LLM providers into OpenWebUI.
It enables seamless use of models like OpenAI, Mistral, Ollama, Anthropic, and others via one central proxy configuration.

# openwebui-litellm-proxy

ğŸ‡©ğŸ‡ª **OpenWebUI + LiteLLM Proxy Setup**

Dies ist ein Setup-Projekt zur Integration von [LiteLLM](https://github.com/BerriAI/litellm) als universelle API-Schnittstelle fÃ¼r verschiedene LLM-Anbieter in OpenWebUI. Es ermÃ¶glicht die Anbindung von Modellen wie OpenAI, Mistral, Ollama, Anthropic u.â€¯v.â€¯m. Ã¼ber eine zentrale Proxy-Konfiguration.

## ğŸ”§ Features

- ğŸ” Proxy fÃ¼r beliebige LLMs via LiteLLM
- ğŸ¤– UnterstÃ¼tzung fÃ¼r OpenAI, Mistral, Anthropic, Ollama etc.
- âš™ï¸ Konfigurierbar via `.env` + `config.yaml`

## ğŸ“¦ Schnellstart

```bash
git clone <repo-url>
cd openwebui-litellm-proxy
cp .env.example .env
# Bearbeite .env mit deinem OpenAI-Key etc.
python main.py


## ğŸ‡¬ğŸ‡§ English

**openwebui-litellm-proxy** is a setup project to integrate [LiteLLM](https://github.com/BerriAI/litellm) as a **universal API interface** for various LLM providers into **OpenWebUI**.  
It enables connecting models like **OpenAI**, **Mistral**, **Ollama**, **Anthropic**, and many more via a central proxy configuration.

### ğŸ”§ Features

- ğŸ” Proxy for any LLM via LiteLLM  
- ğŸ¤– Support for OpenAI, Mistral, Anthropic, Ollama, and others  
- âš™ï¸ Easily configurable using `.env` & `config.yaml`

### â–¶ï¸ Quickstart

```bash
git clone <REPOSITORY-URL>
cd openwebui-litellm-proxy
cp .env.example .env
# Edit .env with your OpenAI key etc.
python main.py